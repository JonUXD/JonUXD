```{r}
library(NLP)
library(corpus)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(ggthemes)
library(RWeka)

```

```{r}
library(qdapDictionaries)
library(qdapRegex)
library(qdap)
```

```{r}
options(mc.cores=1)
```

##Loading data

```{r}
data_breaches <- read.csv("Data_Breaches_r.csv")
data_breaches <- as.data.frame(data_breaches)
```

```{r}
names(data_breaches)
```




```{r}
# creating the corpus for the n reviews. corpus_review is a collection of the n reviews
corpus_breaches <- VCorpus(VectorSource(data_breaches$Story))
```

##Text Pre-processing

In this part, the corpus created is pre-processed

```{r}
# set stopwords you would like to remove
own_stopwords <- c()
```

```{r}
# converting to lowercase
data_breaches <- tm_map(corpus_breaches, content_transformer(tolower))

# removing punctuation
data_breaches <- tm_map(data_breaches, removePunctuation)

#removing numbers from text
data_breaches <- tm_map(data_breaches, removeNumbers)

# removing stopwords
data_breaches <- tm_map(data_breaches, removeWords, stopwords("english"))

# remove our own stopwords
data_breaches <- tm_map(data_breaches, removeWords, own_stopwords)

# stemming the document
data_breaches <- tm_map(data_breaches, stemDocument)
```

##Document-Term-Matrix
The document-term-matrix counts the number of times a word appear in a document

```{r}
#create the dtm and the tdm
breaches_dtm <- DocumentTermMatrix(data_breaches)
breaches_tdm <- TermDocumentMatrix(data_breaches)
```
##Cluster Dendrogram


```{r}
breaches_tdm2 <- removeSparseTerms(breaches_tdm, sparse = 0.9)
hc <- hclust(d = dist(breaches_tdm2, method = "euclidean"), method = 'complete')
plot(hc)
```

##Word Associations
```{r}
# WORD ASSOCIATIONS
word_assoc <- "govern"
associations <- findAssocs(breaches_tdm, as.String(word_assoc), 0.2)
```


```{r}
# creating associations dataframe
associations_df <- list_vect2df(associations)[, 2:3]
```

```{r}

ggplot(associations_df, aes(y = associations_df[, 1])) + 
  geom_point(aes(x = associations_df[, 2]), data = associations_df, size = 3) +
  ggtitle("Associations to: "+as.String(word_assoc)) +
  theme_gdocs() + theme(text = element_text(size=10)) + labs(x="Association level", y = "Words")
```

##Frequency of the words
```{r}

#convert TDM to matrix. review number on the columns and words on the wows. Values are frequencies
breaches_matrix <- as.matrix(breaches_tdm)
```

```{r}
# sums the frequency of each word in all documents
breaches_term_freq <- rowSums(breaches_matrix)
```
```{r}
# sort by frequency
breaches_term_freq <- sort(breaches_term_freq, decreasing = T)
# view the top 10 most common words
breaches_term_freq[1:20]
```

```{r}
barplot(breaches_term_freq[1:20], col = "steel blue", las = 2)
```

```{r}
breaches_word_freq <- data.frame(term = names(breaches_term_freq), num = breaches_term_freq)
# create wordcloud
wordcloud(breaches_word_freq$term, breaches_word_freq$num, max.words = 50, colors = c("blue", "black", "tomato"))
```


```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm_breaches_bigram = TermDocumentMatrix(data_breaches, control = list(tokenize = BigramTokenizer))

```
##Bi-gram frequency study
```{r}
freq = sort(rowSums(as.matrix(tdm_breaches_bigram)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)
```

```{r}
ggplot(head(freq.df,15), aes(reorder(word,freq), freq)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab("Bigrams") + ylab("Frequency") +
  ggtitle("Most frequent bigrams") + theme(text = element_text(size=20))
```

